{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "B1RItN5nvsAa"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytz in /home/youmna/miniconda3/lib/python3.12/site-packages (2025.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /home/youmna/miniconda3/lib/python3.12/site-packages (3.9.1)\n",
            "Requirement already satisfied: click in /home/youmna/miniconda3/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /home/youmna/.local/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/youmna/miniconda3/lib/python3.12/site-packages (from nltk) (2025.7.34)\n",
            "Requirement already satisfied: tqdm in /home/youmna/miniconda3/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NGaQGmGwAns",
        "outputId": "ac2bcfb8-7c4b-4c5c-ef9a-5295d89ee9c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/youmna/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/youmna/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/youmna/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/youmna/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# üß† Download required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEjqJq8DwF66",
        "outputId": "fb3577cd-6675-4534-e0cd-71c833b7b382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Original Text:\n",
            " \n",
            "Hey there! ü§ñ This is a sample TEXT for NLP preprocessing. It includes:\n",
            "- UPPERCASE letters\n",
            "- Punctuation!!!\n",
            "- Numbers like 12345\n",
            "- Stopwords such as 'the', 'is', and 'in'\n",
            "\n",
            "Let's CLEAN it all up\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## üìù Sample Input Text\n",
        "text = \"\"\"\n",
        "Hey there! ü§ñ This is a sample TEXT for NLP preprocessing. It includes:\n",
        "- UPPERCASE letters\n",
        "- Punctuation!!!\n",
        "- Numbers like 12345\n",
        "- Stopwords such as 'the', 'is', and 'in'\n",
        "\n",
        "Let's CLEAN it all up\n",
        "\"\"\"\n",
        "\n",
        "print(\"üîπ Original Text:\\n\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "1EaYRzcqwTYy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hey there ü§ñ this is a sample text for nlp preprocessing it includes uppercase letters punctuation numbers like stopwords such as the is and in lets clean it all up'"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "## üßΩ Step 1: Text Cleaning\n",
        "def clean_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "clean_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "igZE6Xt4wUsu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word_tokens: ['Hey', 'there', '!', 'ü§ñ', 'This', 'is', 'a', 'sample', 'TEXT', 'for', 'NLP', 'preprocessing', '.', 'It', 'includes', ':', '-', 'UPPERCASE', 'letters', '-', 'Punctuation', '!', '!', '!', '-', 'Numbers', 'like', '12345', '-', 'Stopwords', 'such', 'as', \"'the\", \"'\", ',', \"'is\", \"'\", ',', 'and', \"'in\", \"'\", 'Let', \"'s\", 'CLEAN', 'it', 'all', 'up']\n",
            "sentence_tokens: ['\\nHey there!', 'ü§ñ This is a sample TEXT for NLP preprocessing.', 'It includes:\\n- UPPERCASE letters\\n- Punctuation!!!', \"- Numbers like 12345\\n- Stopwords such as 'the', 'is', and 'in'\\n\\nLet's CLEAN it all up\"]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/youmna/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /home/youmna/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "## ‚úÇÔ∏è Step 2: Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "sentence_tokens = nltk.sent_tokenize(text)\n",
        "\n",
        "print(\"word_tokens:\",word_tokens)\n",
        "print(\"sentence_tokens:\",sentence_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "giu05QiUwiAI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "filtered text: ['Hey', '!', 'ü§ñ', 'sample', 'TEXT', 'NLP', 'preprocessing', '.', 'includes', ':', '-', 'UPPERCASE', 'letters', '-', 'Punctuation', '!', '!', '!', '-', 'Numbers', 'like', '12345', '-', 'Stopwords', \"'the\", \"'\", ',', \"'is\", \"'\", ',', \"'in\", \"'\", 'Let', \"'s\", 'CLEAN']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/youmna/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "## üö´ Step 3: Stopword Removal\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "filtered_words = [w for w in words if w.lower() not in stop_words]\n",
        "\n",
        "print(\"filtered text:\", filtered_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "hmkKQxEdwi95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hey', '!', 'ü§ñ', 'sampl', 'text', 'nlp', 'preprocess', '.', 'includ', ':', '-', 'uppercas', 'letter', '-', 'punctuat', '!', '!', '!', '-', 'number', 'like', '12345', '-', 'stopword', \"'the\", \"'\", ',', \"'i\", \"'\", ',', \"'in\", \"'\", 'let', \"'s\", 'clean']\n"
          ]
        }
      ],
      "source": [
        "## üå± Step 4: Stemming\n",
        "stemmer= PorterStemmer()\n",
        "stemmed_words=[stemmer.stem(w)for w in filtered_words]\n",
        "print(stemmed_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "lKaScGE2wmfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hey', '!', 'ü§ñ', 'sample', 'TEXT', 'NLP', 'preprocessing', '.', 'includes', ':', '-', 'UPPERCASE', 'letter', '-', 'Punctuation', '!', '!', '!', '-', 'Numbers', 'like', '12345', '-', 'Stopwords', \"'the\", \"'\", ',', \"'is\", \"'\", ',', \"'in\", \"'\", 'Let', \"'s\", 'CLEAN']\n"
          ]
        }
      ],
      "source": [
        "## üçÉ Step 5: Lemmatization\n",
        "lemmatizer= WordNetLemmatizer()\n",
        "lemmatized_words=[lemmatizer.lemmatize(w)for w in filtered_words]\n",
        "print(lemmatized_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "-p2S58s3wozm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ Count Vectorizer Vocabulary:\n",
            " ['12345' 'clean' 'hey' 'in' 'includ' 'let' 'letter' 'like' 'nlp' 'number'\n",
            " 'preprocess' 'punctuat' 'sampl' 'stopword' 'text' 'the' 'uppercas']\n",
            "üîπ Count Matrix:\n",
            " [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "\n",
            "üîπ TF-IDF Vocabulary:\n",
            " ['12345' 'clean' 'hey' 'in' 'includ' 'let' 'letter' 'like' 'nlp' 'number'\n",
            " 'preprocess' 'punctuat' 'sampl' 'stopword' 'text' 'the' 'uppercas']\n",
            "üîπ TF-IDF Matrix:\n",
            " [[0.24253563 0.24253563 0.24253563 0.24253563 0.24253563 0.24253563\n",
            "  0.24253563 0.24253563 0.24253563 0.24253563 0.24253563 0.24253563\n",
            "  0.24253563 0.24253563 0.24253563 0.24253563 0.24253563]]\n",
            "\n",
            "üîπ Count Matrix (Lemmatization):\n",
            " [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "\n",
            "üîπ TF-IDF Matrix (Lemmatization):\n",
            " [[0.23570226 0.23570226 0.23570226 0.23570226 0.23570226 0.23570226\n",
            "  0.23570226 0.23570226 0.23570226 0.23570226 0.23570226 0.23570226\n",
            "  0.23570226 0.23570226 0.23570226 0.23570226 0.23570226 0.23570226]]\n"
          ]
        }
      ],
      "source": [
        "## üìä Step 6: Vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "stemmed_text = \" \".join(stemmed_words)\n",
        "\n",
        "# Count Vectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_matrix = count_vectorizer.fit_transform([stemmed_text])\n",
        "\n",
        "print(\"\\nüîπ Count Vectorizer Vocabulary:\\n\", count_vectorizer.get_feature_names_out())\n",
        "print(\"üîπ Count Matrix:\\n\", count_matrix.toarray())\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([stemmed_text])\n",
        "\n",
        "print(\"\\nüîπ TF-IDF Vocabulary:\\n\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"üîπ TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
        "\n",
        "lemmatized_text = \" \".join(lemmatized_words)\n",
        "\n",
        "count_matrix_lem = count_vectorizer.fit_transform([lemmatized_text])\n",
        "print(\"\\nüîπ Count Matrix (Lemmatization):\\n\", count_matrix_lem.toarray())\n",
        "\n",
        "tfidf_matrix_lem = tfidf_vectorizer.fit_transform([lemmatized_text])\n",
        "print(\"\\nüîπ TF-IDF Matrix (Lemmatization):\\n\", tfidf_matrix_lem.toarray())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
